---
sidebar_position: 3
slug: /steel-thread-intro
---

# ğŸ§µ Introducing Steel Thread

Steel Thread is a lightweight, extensible framework for evaluating LLM agents â€” designed to help teams measure quality, catch regressions, and improve performance with minimal friction.

It supports **offline** and **online evals**, integrates deeply with Portia Cloud, and is built from the ground up for **real-world agentic workflows**.

---

## ğŸ§  Why We Built Steel Thread

Evaluating agents isnâ€™t hard because the models are bad â€” itâ€™s hard because:

- The output space is non-deterministic
- The tool usage is complex and multi-step
- The definition of "correct" is subjective
- And most of all: **curating test data is painful**

We found that most eval frameworks fall down not on logic or metrics â€” but on data. They assume someone else is maintaining clean eval datasets.

Thatâ€™s the bottleneck.

So we flipped the problem on its head.

Instead of asking teams to build new datasets from scratch, Steel Thread **plugs directly into the data you already generate in Portia Cloud**:

- Plans
- Plan Runs
- Tool Calls
- User IDs
- Metadata and outputs

Now, every agent execution can become an eval â€” either retrospectively or in real time.

---

## âš™ï¸ What Does It Do?

Steel Thread helps you answer the question:

> "Is my agent getting better or worse?"

It does this by providing:

### âœ… Offline Evals
Run against curated static datasets. Useful for:
- Iterating on prompts
- Testing new toolchains
- Benchmarking models
- Catching regressions before deployment

### ğŸ“ˆ Online Evals
Run against your live or recent production runs. Useful for:
- Monitoring quality in real usage
- Tracking performance across time or model changes
- Detecting silent failures

### ğŸ¯ Custom Metrics
Use rules, thresholds, or even LLMs-as-judges to compute:
- Accuracy
- Completeness
- Clarity
- Efficiency
- Latency
- Tool usage
- ...or domain-specific checks

---

## ğŸ”Œ Built for Portia

Steel Thread is deeply integrated with the Portia agentic platform.

It works natively with:
- **Plan and PlanRun IDs**
- **ToolCall metadata**
- **End user context**
- **Agent outputs (e.g. final outputs, intermediate values)**
- **APIs and UI features in Portia Cloud**

This means you donâ€™t need to create new test harnesses or annotate synthetic datasets â€” you can evaluate what's already happening.

Just point Steel Thread at your Portia instance, and start measuring.

---

## ğŸ§© Flexible & Extensible

Steel Thread is designed to be modular:

- âœ… Drop in custom metrics
- ğŸ› ï¸ Stub or override tool behavior
- ğŸ”„ Run in CI or ad hoc from the CLI
- ğŸ” Mix and match online and offline strategies
- ğŸ“Š Save metrics wherever you like â€” log, database, dashboard

It plays well with teams at any stage of maturity â€” whether youâ€™re just getting started with agents or deploying them in production.

---

## ğŸš€ Get Started

Once installed, you can start running evals in just a few lines:

```python
from steelthread.steelthread import SteelThread, OnlineEvalConfig
from portia import Config

config = Config.from_default()
SteelThread().run_online(
    OnlineEvalConfig(data_set_name="prod-evals", config=config)
)
````

Or, define a custom offline dataset with your own metrics and stubs:

```python
from steelthread.offline_evaluators.evaluator import OfflineEvaluator
from steelthread.metrics.metric import Metric

class MyEvaluator(OfflineEvaluator):
    def eval_test_case(self, test_case, plan_run, metadata):
        return Metric(name="custom", score=1.0, description="Always passes!")
```

---

## ğŸ§¬ Why Itâ€™s Different

Steel Thread isnâ€™t just another eval runner. Itâ€™s an opinionated framework focused on:

* Using your **real production data**
* Supporting **deep introspection** into agent behavior
* Making evals **easy to write and easy to trust**

We believe the best way to scale intelligent agents is not just to deploy them â€” but to hold them accountable.

Steel Thread helps you do just that.

