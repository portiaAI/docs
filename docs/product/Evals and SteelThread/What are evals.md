---
sidebar_position: 1
slug: /what-are-evals
---

# ğŸ§ª What Are Evals?

As language models and agentic systems become more powerful, **evaluating** their behavior becomes increasingly critical.

Unlike traditional software, LLM-based systems:
- Donâ€™t have deterministic outputs
- Can solve tasks in many valid ways
- May rely on external tools or memory
- Are often judged by human-like reasoning, not simple correctness

Thatâ€™s why we need structured **evaluations**, or **evals**.

---

## ğŸ¯ What Are Evals?

An **eval** is a test case that measures how well a system performs a task. It usually includes:

1. **An input** â€” like a prompt, question, or plan.
2. **An expected behavior** â€” what a good answer or process looks like.
3. **One or more metrics** â€” quantifying quality, correctness, efficiency, or alignment.

Evals let us ask:
- Did the model produce a useful output?
- Did it follow the right steps?
- Did it use the right tools?
- Did it complete in a reasonable time?
- Did it behave consistently across changes?

---

## ğŸ” Why Are Evals Important?

LLMs and agents don't fail like traditional code. Instead, they:

- Hallucinate answers
- Misuse tools
- Produce results that seem plausible but are subtly wrong
- Behave inconsistently with small input shifts

These aren't bugs â€” theyâ€™re behavioral drift. And without evals, theyâ€™re nearly impossible to detect at scale.

Evals provide:

- âœ… Confidence before deployment
- ğŸ“‰ Regression detection during iteration
- ğŸ“Š Benchmarks for tracking improvement
- ğŸ§ª Insight into how your system behaves under stress or edge cases

---

## ğŸ§© What Makes Evaluation Hard?

Evaluating agents is different from testing conventional software.

| Challenge                  | Why It Matters                                      |
|---------------------------|-----------------------------------------------------|
| ğŸ§  Multiple valid outputs | Thereâ€™s rarely one â€œcorrectâ€ answer                 |
| ğŸ” Stateful / multi-step  | Behavior depends on memory, history, and tool use   |
| ğŸ§° Tool use & side effects| Execution isnâ€™t just text â€” it may call APIs/tools  |
| ğŸ¤– Subjective quality     | Human judgment often needed to assess usefulness    |
| ğŸ” Evolving models        | Model versions can subtly shift behavior            |

You canâ€™t write a single `assert output == expected` test and be done. You need a broader framework to evaluate performance meaningfully.

---

## ğŸ“ Types of Metrics

Good evals result in **metrics** â€” numerical scores that help track progress or catch regressions.

Examples include:

- **Correctness** â€” Did the system return the right answer?
- **Completeness** â€” Were all relevant points included?
- **Clarity** â€” Was the output easy to follow?
- **Efficiency** â€” Were unnecessary steps avoided?
- **Latency** â€” How long did it take?
- **Tool usage** â€” Did it use the expected tools?

These can be generated by:
- âœï¸ Rule-based checks (e.g., string match)
- ğŸ’¬ LLM judges ("Does this answer the question well?")
- ğŸ§ª Custom logic specific to your domain

---

## ğŸ§ª Evals vs. Unit Tests

| Unit Tests                | Evals                                     |
|---------------------------|-------------------------------------------|
| Binary pass/fail          | Often graded on a scale (0.0 to 1.0)       |
| Deterministic             | Tolerate variability                      |
| Focused on internal logic | Focused on observable behavior            |
| Fail fast                 | Interpret trends over many cases          |
| Used in all software      | Essential for ML/LLM/agent-based systems  |

Both are valuable â€” but for anything involving LLMs, evals are essential.

---

## ğŸ“ Who Should Use Evals?

Anyone building systems that rely on:

- Language models (chatbots, assistants, generators)
- Agent frameworks (planners + tools)
- Automated workflows
- Retrieval-augmented generation (RAG)
- Chain-of-thought reasoning

Evals let you move from **guessing** to **measuring** â€” replacing intuition with evidence.

---