---
sidebar_position: 1
slug: /what-are-evals
---

# 🧪 What Are Evals?

As language models and agentic systems become more powerful, **evaluating** their behavior becomes increasingly critical.

Unlike traditional software, LLM-based systems:
- Don’t have deterministic outputs
- Can solve tasks in many valid ways
- May rely on external tools or memory
- Are often judged by human-like reasoning, not simple correctness

That’s why we need structured **evaluations**, or **evals**.

---

## 🎯 What Are Evals?

An **eval** is a test case that measures how well a system performs a task. It usually includes:

1. **An input** — like a prompt, question, or plan.
2. **An expected behavior** — what a good answer or process looks like.
3. **One or more metrics** — quantifying quality, correctness, efficiency, or alignment.

Evals let us ask:
- Did the model produce a useful output?
- Did it follow the right steps?
- Did it use the right tools?
- Did it complete in a reasonable time?
- Did it behave consistently across changes?

---

## 🔍 Why Are Evals Important?

LLMs and agents don't fail like traditional code. Instead, they:

- Hallucinate answers
- Misuse tools
- Produce results that seem plausible but are subtly wrong
- Behave inconsistently with small input shifts

These aren't bugs — they’re behavioral drift. And without evals, they’re nearly impossible to detect at scale.

Evals provide:

- ✅ Confidence before deployment
- 📉 Regression detection during iteration
- 📊 Benchmarks for tracking improvement
- 🧪 Insight into how your system behaves under stress or edge cases

---

## 🧩 What Makes Evaluation Hard?

Evaluating agents is different from testing conventional software.

| Challenge                  | Why It Matters                                      |
|---------------------------|-----------------------------------------------------|
| 🧠 Multiple valid outputs | There’s rarely one “correct” answer                 |
| 🔁 Stateful / multi-step  | Behavior depends on memory, history, and tool use   |
| 🧰 Tool use & side effects| Execution isn’t just text — it may call APIs/tools  |
| 🤖 Subjective quality     | Human judgment often needed to assess usefulness    |
| 🔁 Evolving models        | Model versions can subtly shift behavior            |

You can’t write a single `assert output == expected` test and be done. You need a broader framework to evaluate performance meaningfully.

---

## 📐 Types of Metrics

Good evals result in **metrics** — numerical scores that help track progress or catch regressions.

Examples include:

- **Correctness** — Did the system return the right answer?
- **Completeness** — Were all relevant points included?
- **Clarity** — Was the output easy to follow?
- **Efficiency** — Were unnecessary steps avoided?
- **Latency** — How long did it take?
- **Tool usage** — Did it use the expected tools?

These can be generated by:
- ✍️ Rule-based checks (e.g., string match)
- 💬 LLM judges ("Does this answer the question well?")
- 🧪 Custom logic specific to your domain

---

## 🧪 Evals vs. Unit Tests

| Unit Tests                | Evals                                     |
|---------------------------|-------------------------------------------|
| Binary pass/fail          | Often graded on a scale (0.0 to 1.0)       |
| Deterministic             | Tolerate variability                      |
| Focused on internal logic | Focused on observable behavior            |
| Fail fast                 | Interpret trends over many cases          |
| Used in all software      | Essential for ML/LLM/agent-based systems  |

Both are valuable — but for anything involving LLMs, evals are essential.

---

## 🎓 Who Should Use Evals?

Anyone building systems that rely on:

- Language models (chatbots, assistants, generators)
- Agent frameworks (planners + tools)
- Automated workflows
- Retrieval-augmented generation (RAG)
- Chain-of-thought reasoning

Evals let you move from **guessing** to **measuring** — replacing intuition with evidence.

---