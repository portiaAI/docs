---
sidebar_position: 2
slug: /offline-online-evals
---

# ðŸ’» Offline vs Online Evals

When working with LLM-based systems and agents, it's not enough to evaluate performance once â€” you need **ongoing, structured feedback**. This is where **offline** and **online** evals come in.

Each serves a different purpose. Together, they form a complete picture of how your system behaves â€” both in development and in the wild.

---

## ðŸ“¦ What Are Offline Evals?

**Offline evals** are static, curated datasets designed to be run repeatedly during development.

- âœ… **Predictable**: You run them on known inputs and expected behaviors.
- ðŸ” **Repeatable**: You can rerun the same test set after any code, prompt, or model change.
- ðŸ§ª **Controlled**: You isolate variables to see exactly what changed and why.

### Common Use Cases
- Testing changes to prompts or logic
- Benchmarking a new model
- Regression testing before deploys
- Developing new evaluators or metrics

> Think of offline evals as your **unit tests and benchmarks** for LLM agents.

---

## ðŸŒ What Are Online Evals?

**Online evals** are dynamic evaluations that operate over live system data â€” your real-time or recent plans and executions.

- ðŸš€ **Production-aware**: They track real user traffic and system behavior.
- ðŸ§­ **Continuous**: As new plans and runs are created, theyâ€™re automatically evaluated.
- ðŸ§© **Unfiltered**: They expose blind spots not covered by test datasets.

### Common Use Cases
- Monitoring quality in production
- Detecting silent failures or regressions
- Measuring alignment with user goals
- Spotting drift over time (e.g., LLM or data changes)

> Think of online evals as your **observability layer** for agents in the real world.

---

## ðŸ” Key Differences

| Feature            | Offline Evals                        | Online Evals                          |
|--------------------|--------------------------------------|----------------------------------------|
| Input Source       | Static, curated test cases           | Live or recent production data         |
| Frequency          | Manually or on CI                    | Continuous or scheduled                |
| Use Case           | Development, testing, iteration      | Monitoring, regression, drift detection|
| Control            | High â€” inputs & expectations known   | Low â€” inputs and outputs are emergent  |
| Scope              | Targeted tasks and edge cases        | Real-world coverage                    |

---

## ðŸ§  Why We Need Both

Relying on just one is risky:

- **Only offline** means youâ€™re blind to real-world edge cases, user behavior, and model drift.
- **Only online** means you lose the precision and control needed to improve the system safely.

Used together, they give you:

- Confidence in what you **changed**
- Visibility into whatâ€™s **happening**
- Tools to **iterate intelligently** and **respond quickly**

---

## ðŸ¤” When Should You Use Each?

| Scenario                                              | Use This Eval Type        |
|-------------------------------------------------------|---------------------------|
| Refactoring prompts                                   | ðŸ§ª Offline                |
| Validating new tool logic                             | ðŸ§ª Offline                |
| Benchmarking multiple models                          | ðŸ§ª Offline                |
| Tracking production quality over time                 | ðŸŒ Online                |
| Detecting drift from user feedback                    | ðŸŒ Online                |
| Diagnosing regressions after a release                | ðŸŒ + ðŸ§ª Both             |
| Building dashboards or leaderboards                   | ðŸŒ Online                |

---

## ðŸ§¬ Evals as a Lifecycle

- **Offline evals** keep your development grounded.
- **Online evals** keep your system honest in production.

Together, they support a feedback loop thatâ€™s essential for intelligent, adaptable agents.

> Treat evals as part of your build-measure-learn cycle â€” not an afterthought.

