---
sidebar_position: 7
slug: /st-stubs
---

# ðŸ› ï¸ Tool Stubbing for Reliable Evals

When running offline evals, your agent may call tools like â€œweatherâ€, â€œsearchâ€, or â€œlookup_customerâ€. If those tools hit live systems, you'll get non-deterministic results â€” which can make evaluation noisy and inconsistent.

To solve this, **Steel Thread** supports stubbing tools with fixed, fake responses.

This makes your tests:

- âœ… **Deterministic** â€” the same input always produces the same output
- âœ… **Isolated** â€” no external API calls or flaky systems
- âœ… **Repeatable** â€” easy to track regressions across changes

---

## ðŸ¤” When to Stub

Use tool stubs when:

- You're writing **offline evaluations**
- The tool response **affects the plan or output**
- You want **consistent scoring** across iterations

---

## âš™ï¸ How Tool Stubbing Works

Steel Thread provides a `ToolStubRegistry` â€” a drop-in replacement for Portiaâ€™s default registry.

You can wrap your existing tools and selectively override individual tools by ID.

---

### ðŸ” Example: Stubbing the Weather Tool

```python
from portia import Config, Portia, ToolRunContext, DefaultToolRegistry
from steelthread.portia.tools import ToolStubRegistry

# Define stub behavior
def weather_stub(index, ctx: ToolRunContext, args, kwargs):
    city = kwargs.get("city", "").lower()
    return {
        "sydney": "33.28",
        "london": "2.00",
    }.get(city, f"Unknown city: {city}")

# Wrap your registry with a stub registry
portia = Portia(
    config,
    tools=ToolStubRegistry(
        DefaultToolRegistry(config),
        stubs={
            "weather_tool": weather_stub,
        },
    )
)
````

> âœ… The rest of your tools still work as normal â€” only the stubbed one is overridden.

---

## ðŸ”¬ Tool Stub Function Signature

Tool stubs are simple Python functions:

```python
def my_stub(
    tool_call_index: int,
    ctx: ToolRunContext,
    args: tuple,
    kwargs: dict,
) -> Any:
    ...
```

You can use:

* `tool_call_index` to vary behavior across iterations
* `ctx` to access plan metadata, end user, etc.
* `args` and `kwargs` for tool inputs

Return any value your real tool would return â€” a string, object, dict, etc.

---

## ðŸ§ª Using Tool Stubs in Offline Eval Runs

```python
from steelthread.steelthread import SteelThread, OfflineEvalConfig

SteelThread().run_offline(
    portia,
    OfflineEvalConfig(
        data_set_name="offline_eval_v1",
        config=config,
        iterations=3,
    )
)
```

With the stubbed tool in place, your offline evals will be clean, fast, and reproducible.

---

## âœ… Best Practices

* âœ… Stub only the tools that matter for evaluation
* âœ… Use consistent return types (e.g. same as real tool)
* âœ… Use `tool_call_index` if you want per-run variance
* âœ… Combine stubbing with assertions to detect misuse (e.g. tool called too many times)

---
